[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Crime Trends and Socioeconomic Analysis in Philadelphia",
    "section": "",
    "text": "In this project, we delve into the intricate relationship between crime trends and socioeconomic factors in Philadelphia. Utilizing advanced analytical techniques, we aim to uncover patterns and insights that can help in understanding and potentially mitigating crime in urban areas.\nOur analysis is powered by Quarto, enabling us to integrate Python code seamlessly with our narrative, and our findings are shared online via GitHub Pages. This platform allows us to present a comprehensive and interactive exploration of crime data and socioeconomic variables.\n\n\n\n\n\n\nNote\n\n\n\nOur project is a culmination of extensive research and data analysis. While this site serves as a primary mode of dissemination, we encourage viewers to explore the data and methodologies in depth for a richer understanding."
  },
  {
    "objectID": "index.html#welcome-to-our-analysis",
    "href": "index.html#welcome-to-our-analysis",
    "title": "Crime Trends and Socioeconomic Analysis in Philadelphia",
    "section": "",
    "text": "In this project, we delve into the intricate relationship between crime trends and socioeconomic factors in Philadelphia. Utilizing advanced analytical techniques, we aim to uncover patterns and insights that can help in understanding and potentially mitigating crime in urban areas.\nOur analysis is powered by Quarto, enabling us to integrate Python code seamlessly with our narrative, and our findings are shared online via GitHub Pages. This platform allows us to present a comprehensive and interactive exploration of crime data and socioeconomic variables.\n\n\n\n\n\n\nNote\n\n\n\nOur project is a culmination of extensive research and data analysis. While this site serves as a primary mode of dissemination, we encourage viewers to explore the data and methodologies in depth for a richer understanding."
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Crime Trends and Socioeconomic Analysis in Philadelphia",
    "section": "Project Overview",
    "text": "Project Overview\nHere, we provide a brief overview of our projectâ€™s objectives, the datasets used, and the analytical methods employed.\n\nObjective: Our main goal is to understand how various socioeconomic factors correlate with crime trends in Philadelphia.\nData Sources: We utilize public datasets from the Philadelphia Police Department, U.S. Census data, and other relevant sources.\nMethodology: Our approach includes data cleaning, exploratory data analysis, statistical modeling, and spatial analysis using Python."
  },
  {
    "objectID": "index.html#in-depth-analysis",
    "href": "index.html#in-depth-analysis",
    "title": "Crime Trends and Socioeconomic Analysis in Philadelphia",
    "section": "In-Depth Analysis",
    "text": "In-Depth Analysis\nThis section dives deep into the specifics of our analysis. We break down our approach into several key components:\n\nData Preparation and Cleaning: Details on how we processed the raw data.\nExploratory Data Analysis (EDA): Initial findings and patterns observed from the data.\nStatistical Analysis: Results from various statistical tests and models.\nSpatial Analysis: Geographic patterns and insights derived from the data."
  },
  {
    "objectID": "analysis/ml.html",
    "href": "analysis/ml.html",
    "title": "Clustering and Prediction",
    "section": "",
    "text": "Attempt to cluster police districts by crime data\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport numpy as np\n%matplotlib inline\nincidents = pd.read_csv('../data/crime.csv')\nincidents.head()\n\n\n\n\n\n\n\n\nDc_Dist\nPsa\nDispatch_Date_Time\nDispatch_Date\nDispatch_Time\nHour\nDc_Key\nLocation_Block\nUCR_General\nText_General_Code\nPolice_Districts\nMonth\nLon\nLat\n\n\n\n\n0\n18\n3\n2009-10-02 14:24:00\n2009-10-02\n14:24:00\n14\n200918067518\nS 38TH ST / MARKETUT ST\n800.0\nOther Assaults\nNaN\n2009-10\nNaN\nNaN\n\n\n1\n14\n1\n2009-05-10 00:55:00\n2009-05-10\n00:55:00\n0\n200914033994\n8500 BLOCK MITCH\n2600.0\nAll Other Offenses\nNaN\n2009-05\nNaN\nNaN\n\n\n2\n25\nJ\n2009-08-07 15:40:00\n2009-08-07\n15:40:00\n15\n200925083199\n6TH CAMBRIA\n800.0\nOther Assaults\nNaN\n2009-08\nNaN\nNaN\n\n\n3\n35\nD\n2009-07-19 01:09:00\n2009-07-19\n01:09:00\n1\n200935061008\n5500 BLOCK N 5TH ST\n1500.0\nWeapon Violations\n20.0\n2009-07\n-75.130477\n40.036389\n\n\n4\n9\nR\n2009-06-25 00:14:00\n2009-06-25\n00:14:00\n0\n200909030511\n1800 BLOCK WYLIE ST\n2600.0\nAll Other Offenses\n8.0\n2009-06\n-75.166350\n39.969532\n\n\n\n\n\n\n\n\ncrime_data_type_dist = incidents.groupby(['UCR_General', 'Dc_Dist']).size()\ncrime_data_type_dist = crime_data_type_dist.apply(int)\ncrime_data_type_dist_df = crime_data_type_dist.to_frame()\ncrime_data_type_dist_pt = pd.pivot_table(crime_data_type_dist_df, index=['UCR_General'], columns=['Dc_Dist'])[0] # Get rid of '0 column'\n\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(data=crime_data_type_dist_pt, annot=True, linewidths=0.1, fmt='g', cmap=\"YlOrRd\", ax=ax)\nplt.title(\"Philadelphia Crime Types By Police District (2006-2015)\")\nplt.xlabel(\"Police District\")\nplt.ylabel(\"Crime Type Code\")\nplt.show()\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Assuming 'incidents' DataFrame is already loaded with crime data\n\n# Identifying crime types\ncrime_types = incidents['Text_General_Code'].unique()\nprint(\"Crime Types:\", crime_types)\n\n# Feature engineering: pivot table with crime types as columns, police districts as rows\ncrime_features = incidents.pivot_table(index='Dc_Dist', columns='Text_General_Code', \n                                       values='Dc_Key', aggfunc='count', fill_value=0)\n\n# Data preprocessing: standardization of features\nscaler = StandardScaler()\ncrime_features_scaled = scaler.fit_transform(crime_features)\n\n# Perform K-Means clustering\nkmeans = KMeans(n_clusters=5, random_state=42)\ndistrict_clusters = kmeans.fit_predict(crime_features_scaled)\n\n# Adding the cluster labels to the original DataFrame\ncrime_features['Cluster'] = district_clusters\n\n# Selecting two crime types for visualization (replace with actual crime types)\n# For example, using the first two crime types from the list\nif len(crime_types) &gt;= 2:\n    crime_type_1 = crime_types[0]\n    crime_type_2 = crime_types[1]\n\n    sns.scatterplot(data=crime_features, x=crime_type_1, y=crime_type_2, hue='Cluster', palette='viridis')\n    plt.title('Police District Clusters Based on Crime Data')\n    plt.xlabel(crime_type_1)\n    plt.ylabel(crime_type_2)\n    plt.legend(title='Cluster')\n    plt.show()\nelse:\n    print(\"Not enough crime types for visualization.\")\n\n\nCrime Types: ['Other Assaults' 'All Other Offenses' 'Weapon Violations' 'Thefts'\n 'Burglary Non-Residential' 'Aggravated Assault Firearm'\n 'Theft from Vehicle' 'Disorderly Conduct' 'Vandalism/Criminal Mischief'\n 'Arson' 'Fraud' 'Robbery No Firearm' 'Vagrancy/Loitering'\n 'Motor Vehicle Theft' 'Recovered Stolen Motor Vehicle' 'Robbery Firearm'\n 'Embezzlement' 'Rape' 'DRIVING UNDER THE INFLUENCE'\n 'Forgery and Counterfeiting' 'Narcotic / Drug Law Violations'\n 'Burglary Residential' 'Other Sex Offenses (Not Commercialized)'\n 'Liquor Law Violations' 'Aggravated Assault No Firearm'\n 'Homicide - Criminal' 'Gambling Violations'\n 'Prostitution and Commercialized Vice' 'Public Drunkenness'\n 'Receiving Stolen Property' 'Homicide - Gross Negligence'\n 'Offenses Against Family and Children' 'Homicide - Justifiable' nan]\n\n\n/Users/amandah/Desktop/gopipelines/.conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\nimport geopandas as gpd\n\n# Load the GeoJSON file\ngdf = gpd.read_file('../data/Boundaries_District.geojson')\n\n# Convert 'DISTRICT_' in gdf to the same type as the index in crime_features\ngdf['DISTRICT_'] = gdf['DISTRICT_'].astype(crime_features.index.dtype)\n\n# Join the GeoDataFrame with the cluster labels without setting 'DISTRICT_' as index first\ngdf_merged = gdf.merge(crime_features[['Cluster']], left_on='DISTRICT_', right_index=True)\n\n# Checking the first few rows to ensure the join worked\ngdf_merged.head()\n\n\n\n\n\n\n\n\nOBJECTID\nAREA\nPERIMETER\nDISTRICT_\nDISTRICT_ID\nDIST_NUM\nSUM_AREA\nDIST_NUMC\nLOCATION\nPHONE\nDIV_CODE\nAREA_SQMI\nShape__Area\nShape__Length\ngeometry\nCluster\n\n\n\n\n0\n343\nNone\n69282.588463\n16\nNone\n16\nNone\n16\n39th St. & Lancaster Ave.\n686-3160\nSWPD\n1.216700e+08\n1.927226e+07\n27575.079183\nPOLYGON ((-75.19957 40.00912, -75.20097 40.009...\n4\n\n\n1\n344\nNone\n33150.154961\n17\nNone\n17\nNone\n17\n20th St. & Federal St.\n686-3170\nSPD\n5.786368e+07\n9.154950e+06\n13179.953350\nPOLYGON ((-75.16599 39.94184, -75.16585 39.942...\n4\n\n\n2\n345\nNone\n54403.930038\n18\nNone\n18\nNone\n18\n55th St. & Pine St.\n686-3180\nSWPD\n9.881929e+07\n1.564117e+07\n21629.610687\nPOLYGON ((-75.18466 39.94851, -75.18368 39.949...\n1\n\n\n3\n346\nNone\n51597.051603\n35\nNone\n35\nNone\n35\nN. Broad St. & Champlost St.\n686-3350\nNWPD\n1.544309e+08\n2.450685e+07\n20554.869138\nPOLYGON ((-75.13011 40.05789, -75.13307 40.059...\n1\n\n\n4\n347\nNone\n58075.014444\n39\nNone\n39\nNone\n39\n22nd St. & Hunting Park Ave.\n686-3390\nNWPD\n1.578976e+08\n2.503787e+07\n23123.580262\nPOLYGON ((-75.17892 40.03175, -75.17908 40.031...\n1\n\n\n\n\n\n\n\n\nimport folium\n\nmap_center_latitude = 40.0  # Replace with the actual latitude\nmap_center_longitude = -75.1  # Replace with the actual longitude\n\n# Create a base map\nm = folium.Map(location=[map_center_latitude, map_center_longitude], zoom_start=11)\n\n# Plot using Folium\nfolium.Choropleth(\n    geo_data=gdf_merged.to_json(),\n    data=gdf_merged,\n    columns=['DISTRICT_', 'Cluster'],\n    key_on='feature.properties.DISTRICT_',\n    fill_color='YlOrRd',\n    legend_name='Crime Data Clusters'\n).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nAttempt to predict number of incidents in each police district in each year\n\nincidents['Dispatch_Date'] = pd.to_datetime(incidents['Dispatch_Date'])\n\nincidents['Year'] = incidents['Dispatch_Date'].dt.year\n\nincidents['Year'] = pd.to_datetime(incidents['Dispatch_Date']).dt.year\nincidents['Dc_Dist'] = incidents['Dc_Dist'].astype(str)\n\n# Aggregate the data\ncases_per_year_district = incidents[incidents['Year'] != 2016].groupby(['Year', 'Dc_Dist']).size().unstack(fill_value=0)\n\nfrom sklearn.linear_model import LinearRegression\n\n# Prepare training data\nX_train = cases_per_year_district.index.values.reshape(-1, 1)  # Years as features\ny_train = cases_per_year_district.values  # Cases as target\n\n# Initialize and train the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict for 2016\nX_test = np.array([[2016]])\ny_pred = model.predict(X_test)\n\n# Add the predictions to the DataFrame\ncases_per_year_district.loc[2016] = y_pred.flatten()\n\n# Display the predictions\ncases_per_year_district.loc[2016]\n\nDc_Dist\n1      2511.950376\n12     7688.843609\n14     6726.103759\n15    10659.777444\n16     4391.971429\n17     3621.457143\n18     6582.968421\n19    10320.673684\n2      7320.962406\n22     9907.616541\n23    -1577.685714\n24    10177.912782\n25     7313.508271\n26     5145.091729\n3      7174.001504\n35     7068.842105\n39     6000.413534\n4     -1605.079699\n5      1464.267669\n6      5390.288722\n7      2532.944361\n77      492.009023\n8      3766.572932\n9      5160.864662\n92     -122.899248\nName: 2016, dtype: float64\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n\n# Assuming 'cases_per_year_district' is a DataFrame where each column represents a district\n\nresults = {}\nfor district in cases_per_year_district.columns:\n    # Prepare the data for each district\n    X = cases_per_year_district.index.values.reshape(-1, 1)  # Years as features\n    y = cases_per_year_district[district].values  # Cases for the district as target\n\n    # Initialize the model\n    model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n    # Perform cross-validation\n    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n    rmse_scores = np.sqrt(-scores)\n\n    # Store the results\n    results[district] = {\n        'Mean RMSE': np.mean(rmse_scores),\n        'Std RMSE': np.std(rmse_scores)\n    }\n\n    # Train the model on all available data (excluding 2016) and predict for 2016\n    model.fit(X, y)\n    y_pred = model.predict(np.array([[2016]]))\n    results[district]['2016 Prediction'] = y_pred[0]\n\n# Display the results\nfor district, metrics in results.items():\n    print(f\"District {district}:\")\n    for key, value in metrics.items():\n        print(f\"   {key}: {value}\")\n\nDistrict 1:\n   Mean RMSE: 826.7501332673764\n   Std RMSE: 764.5738827821881\n   2016 Prediction: 2710.2632781954785\nDistrict 12:\n   Mean RMSE: 2187.1723022607803\n   Std RMSE: 2353.52857536613\n   2016 Prediction: 8140.204270676607\nDistrict 14:\n   Mean RMSE: 2063.3850241517202\n   Std RMSE: 2004.17937768756\n   2016 Prediction: 7244.796781955026\nDistrict 15:\n   Mean RMSE: 2999.6699887481673\n   Std RMSE: 3378.6250975352254\n   2016 Prediction: 11486.665308270636\nDistrict 16:\n   Mean RMSE: 1322.6165374967163\n   Std RMSE: 1407.2881223942875\n   2016 Prediction: 4760.458857142825\nDistrict 17:\n   Mean RMSE: 1345.8224358035836\n   Std RMSE: 1036.6577888966335\n   2016 Prediction: 3913.898285714281\nDistrict 18:\n   Mean RMSE: 2257.3841942021813\n   Std RMSE: 2064.0622796698617\n   2016 Prediction: 7238.54663157898\nDistrict 19:\n   Mean RMSE: 2504.770805713046\n   Std RMSE: 3027.7310474166725\n   2016 Prediction: 10922.418526315803\nDistrict 2:\n   Mean RMSE: 1647.7693539095715\n   Std RMSE: 2217.214021844633\n   2016 Prediction: 7648.562180451031\nDistrict 22:\n   Mean RMSE: 2835.7822883151275\n   Std RMSE: 2534.2688438430896\n   2016 Prediction: 10363.596240601513\nDistrict 23:\n   Mean RMSE: 1701.5956078144293\n   Std RMSE: 1307.9073515652385\n   2016 Prediction: -1167.4774285715073\nDistrict 24:\n   Mean RMSE: 3055.1965096175127\n   Std RMSE: 2768.309725644463\n   2016 Prediction: 10821.945458646682\nDistrict 25:\n   Mean RMSE: 3183.663021014357\n   Std RMSE: 2400.532619829073\n   2016 Prediction: 8007.7861203011125\nDistrict 26:\n   Mean RMSE: 1446.7978672382092\n   Std RMSE: 1633.4668398523381\n   2016 Prediction: 5507.547879699215\nDistrict 3:\n   Mean RMSE: 1966.486349873682\n   Std RMSE: 1613.232574494006\n   2016 Prediction: 7237.151112781965\nDistrict 35:\n   Mean RMSE: 2652.687903752947\n   Std RMSE: 2140.50992174316\n   2016 Prediction: 7751.823157894728\nDistrict 39:\n   Mean RMSE: 1880.6899223191467\n   Std RMSE: 1573.2915163861946\n   2016 Prediction: 6431.196015037535\nDistrict 4:\n   Mean RMSE: 1708.345246455047\n   Std RMSE: 1241.5933519080345\n   2016 Prediction: -1187.758977443748\nDistrict 5:\n   Mean RMSE: 608.1171067374104\n   Std RMSE: 397.2782495251096\n   2016 Prediction: 1571.4480751879455\nDistrict 6:\n   Mean RMSE: 1929.571300019159\n   Std RMSE: 1734.7014897765557\n   2016 Prediction: 5951.9136541353355\nDistrict 7:\n   Mean RMSE: 695.3717311730533\n   Std RMSE: 745.4792984782163\n   2016 Prediction: 2706.428827067659\nDistrict 77:\n   Mean RMSE: 193.58382604545855\n   Std RMSE: 128.93056795423246\n   2016 Prediction: 528.9366766917292\nDistrict 8:\n   Mean RMSE: 1152.2518296091398\n   Std RMSE: 1111.0495037560263\n   2016 Prediction: 3992.0839699247736\nDistrict 9:\n   Mean RMSE: 1599.119039355884\n   Std RMSE: 1528.3191707321223\n   2016 Prediction: 5591.939849623954\nDistrict 92:\n   Mean RMSE: 195.96745256862317\n   Std RMSE: 189.4148487758863\n   2016 Prediction: -90.94544360903441\n\n\n\nactual_2016 = cases_per_year_district.loc[2016]\n\n# Retrieve predicted values for 2016\npredicted_2016 = {district: data['2016 Prediction'] for district, data in results.items()}\n\n# Convert to DataFrame\nactual_df = pd.DataFrame(actual_2016).rename(columns={2016: 'Actual'})\npredicted_df = pd.DataFrame(predicted_2016, index=['Predicted']).transpose()\n\n# Combine into a single DataFrame\ncombined_df = actual_df.join(predicted_df)\n\ncombined_df\n\n\n\n\n\n\n\n\nActual\nPredicted\n\n\nDc_Dist\n\n\n\n\n\n\n1\n2511.950376\n2710.263278\n\n\n12\n7688.843609\n8140.204271\n\n\n14\n6726.103759\n7244.796782\n\n\n15\n10659.777444\n11486.665308\n\n\n16\n4391.971429\n4760.458857\n\n\n17\n3621.457143\n3913.898286\n\n\n18\n6582.968421\n7238.546632\n\n\n19\n10320.673684\n10922.418526\n\n\n2\n7320.962406\n7648.562180\n\n\n22\n9907.616541\n10363.596241\n\n\n23\n-1577.685714\n-1167.477429\n\n\n24\n10177.912782\n10821.945459\n\n\n25\n7313.508271\n8007.786120\n\n\n26\n5145.091729\n5507.547880\n\n\n3\n7174.001504\n7237.151113\n\n\n35\n7068.842105\n7751.823158\n\n\n39\n6000.413534\n6431.196015\n\n\n4\n-1605.079699\n-1187.758977\n\n\n5\n1464.267669\n1571.448075\n\n\n6\n5390.288722\n5951.913654\n\n\n7\n2532.944361\n2706.428827\n\n\n77\n492.009023\n528.936677\n\n\n8\n3766.572932\n3992.083970\n\n\n9\n5160.864662\n5591.939850\n\n\n92\n-122.899248\n-90.945444\n\n\n\n\n\n\n\n\ndifferences = {}\nfor district in cases_per_year_district.columns:\n    actual_2016 = cases_per_year_district.loc[2016, district]\n    predicted_2016 = results[district]['2016 Prediction']\n    differences[district] = actual_2016 - predicted_2016\n\n\nimport matplotlib.pyplot as plt\n\n# Convert the differences dictionary to a list for plotting\ndistricts = list(differences.keys())\ndiff_values = list(differences.values())\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nplt.bar(districts, diff_values, color='skyblue')\nplt.xlabel('District')\nplt.ylabel('Difference (Actual - Predicted)')\nplt.title('Difference Between Actual and Predicted Cases in 2016')\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "analysis/crime-year-trends.html",
    "href": "analysis/crime-year-trends.html",
    "title": "Temporal Analysis of Philadelphia Crime Data (2006-2018)",
    "section": "",
    "text": "In this section, we delve into the temporal patterns of crime in Philadelphia, analyzing data spanning from 2006 to 2018. Our objective is to uncover trends over different time scales â€“ by hour, day, month, and year. This analysis will help us understand how crime rates fluctuate over time, revealing patterns that could be pivotal for crime prevention strategies and law enforcement planning.\nWe will explore: - Hourly Trends: How does crime vary throughout a day? - Daily Trends: Are certain days of the week more prone to crime? - Monthly Trends: What are the monthly variations in crime rates? - Yearly Trends: How has crime evolved over the years?\nThis comprehensive temporal analysis aims to provide a detailed overview of the dynamics of crime incidents in Philadelphia, offering insights that go beyond mere numbers to understand the rhythm of the cityâ€™s safety and security challenges.\nData was provided by https://www.opendataphilly.org/\n\nQuick Overview of Crime Data\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport numpy as np\n%matplotlib inline\nincidents = pd.read_csv('../data/crime.csv')\nincidents.head()\n\n\n\n\n\n\n\n\nDc_Dist\nPsa\nDispatch_Date_Time\nDispatch_Date\nDispatch_Time\nHour\nDc_Key\nLocation_Block\nUCR_General\nText_General_Code\nPolice_Districts\nMonth\nLon\nLat\n\n\n\n\n0\n18\n3\n2009-10-02 14:24:00\n2009-10-02\n14:24:00\n14\n200918067518\nS 38TH ST / MARKETUT ST\n800.0\nOther Assaults\nNaN\n2009-10\nNaN\nNaN\n\n\n1\n14\n1\n2009-05-10 00:55:00\n2009-05-10\n00:55:00\n0\n200914033994\n8500 BLOCK MITCH\n2600.0\nAll Other Offenses\nNaN\n2009-05\nNaN\nNaN\n\n\n2\n25\nJ\n2009-08-07 15:40:00\n2009-08-07\n15:40:00\n15\n200925083199\n6TH CAMBRIA\n800.0\nOther Assaults\nNaN\n2009-08\nNaN\nNaN\n\n\n3\n35\nD\n2009-07-19 01:09:00\n2009-07-19\n01:09:00\n1\n200935061008\n5500 BLOCK N 5TH ST\n1500.0\nWeapon Violations\n20.0\n2009-07\n-75.130477\n40.036389\n\n\n4\n9\nR\n2009-06-25 00:14:00\n2009-06-25\n00:14:00\n0\n200909030511\n1800 BLOCK WYLIE ST\n2600.0\nAll Other Offenses\n8.0\n2009-06\n-75.166350\n39.969532\n\n\n\n\n\n\n\n\n\nCrime Trends by Year\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\n\n# Ensure Dispatch_Date is a datetime dtype and Year is extracted as a separate column\nincidents['Dispatch_Date'] = pd.to_datetime(incidents['Dispatch_Date'])\nincidents['Year'] = incidents['Dispatch_Date'].dt.year\n\n# Group by Dispatch_Date and get the count of incidents\nby_date = incidents.groupby(incidents['Dispatch_Date'].dt.to_period(\"D\")).size().reset_index(name='Total')\n\n# Convert Dispatch_Date from Period to DateTime for plotting\nby_date['Dispatch_Date'] = by_date['Dispatch_Date'].dt.to_timestamp()\n\nfig, ax = plt.subplots(figsize=(9, 5.5))\nby_date.plot(x='Dispatch_Date', y='Total', ax=ax, legend=False)\n\n# Formatting the plot to have a similar style to the R ggplot example\nax.set_title('Crimes by Year in Philadelphia (2006-2018)')\nax.set_xlabel('Year')\nax.set_ylabel('Number of Crimes')\nax.grid(True)\n\n# Set x-axis formatter for dates\nax.xaxis.set_major_formatter(DateFormatter('%Y'))\n\nplt.show()\n\n\n\n\n\nSeasonal Patterns: There seem to be regular fluctuations within each year, which could suggest seasonal trends in crime rates Annual Trends: The plot indicate crime rates have decreased over the years in general\n\n\nCrime Trends by Month\n\nincidents['Dispatch_Date'] = pd.to_datetime(incidents['Dispatch_Date'])\nincidents['Year'] = incidents['Dispatch_Date'].dt.year\nincidents['Month'] = incidents['Dispatch_Date'].dt.month\n\n# Group by Month and Year, and count incidents\nmonthly_counts = incidents.groupby(['Year', 'Month']).size().reset_index(name='Total')\n\n# Plotting Crimes by Month over the years\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.lineplot(data=monthly_counts, x='Month', y='Total', hue='Year', marker='o', ax=ax)\n\n# Formatting the plot to have a similar style to the R ggplot example\nax.set_title('Crimes by Month in Philadelphia (2006-2018)')\nax.set_xlabel('Month')\nax.set_ylabel('Total Crimes')\nax.grid(True)\nax.legend(title='Year')\n\nplt.show()\n\n\n\n\nSeasonal Trends: Again, there seems to be a pattern that repeats annually, suggesting seasonal trends in crime rates.\nMonth-to-Month Variability: seems like in winter there are less crime incidents, the peak happens in August, around summer.\n\n\nCrime Trends by Days Within a Week\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\n\nincidents['Dispatch_Date'] = pd.to_datetime(incidents['Dispatch_Date'])\n\n# Extract day of the week from Dispatch_Date (0=Monday, 6=Sunday)\nincidents['DayOfWeek'] = incidents['Dispatch_Date'].dt.dayofweek\n\n# Group by day of the week and count incidents\ndaily_counts = incidents.groupby('DayOfWeek').size().reset_index(name='Total')\n\n# Sort the data by DayOfWeek to maintain the order from Monday to Sunday\ndaily_counts.sort_values('DayOfWeek', inplace=True)\n\n# Plotting Crimes by Day of the Week\nplt.figure(figsize=(10,6))\nsns.barplot(data=daily_counts, x='DayOfWeek', y='Total', palette='viridis')\n\n# Set the x-axis labels to day names\nplt.xticks(range(7), ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n\nplt.title('Crime Trends by Day Within Week in Philadelphia (2006-2018)')\nplt.xlabel('Day of the Week')\nplt.ylabel('Total Crimes')\nplt.grid(True, axis='y')\nplt.show()\n\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_13175/4114548526.py:20: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=daily_counts, x='DayOfWeek', y='Total', palette='viridis')\n\n\n\n\n\nDid not find a pattern here\n\n\nCrime Trends by Days Within a Month\n\nincidents['Dispatch_Date'] = pd.to_datetime(incidents['Dispatch_Date'])\nincidents['DayOfMonth'] = incidents['Dispatch_Date'].dt.day\n\n# Group by day of the month and count incidents\ndaily_counts = incidents.groupby('DayOfMonth').size().reset_index(name='Total')\n\n# Plotting Crimes by Day of the Month\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=daily_counts, x='DayOfMonth', y='Total', marker='o')\n\nplt.title('Crime Trends by Day Within Month in Philadelphia (2006-2018)')\nplt.xlabel('Day of the Month')\nplt.ylabel('Total Crimes')\nplt.grid(True)\nplt.xticks(range(1, 32))  # Set x-ticks to show each day of the month\nplt.tight_layout()  # Adjust layout to prevent clipping of ylabel\nplt.show()\n\n\n\n\n\n\n\nCrimes by Hour\n\nincidents['Dispatch_Date_Time'] = pd.to_datetime(incidents['Dispatch_Date_Time'])\n\n# Extract Year and Hour from Dispatch_Date_Time\nincidents['Year'] = incidents['Dispatch_Date_Time'].dt.year\nincidents['Hour'] = incidents['Dispatch_Date_Time'].dt.hour\n\n# Group by Year to get the total number of crimes each year\nby_year = incidents.groupby('Year').size().reset_index(name='Total')\nby_year['Percent'] = (by_year['Total'] / by_year['Total'].sum()) * 100\n\n# Group by Year and Hour to get the total number of crimes for each hour across each year\nby_hour_year = incidents.groupby(['Year', 'Hour']).size().reset_index(name='Total')\n\n\n# Plotting Crimes by Hour across Years\nplt.figure(figsize=(12, 6))\nfor year in by_hour_year['Year'].unique():\n    subset = by_hour_year[by_hour_year['Year'] == year]\n    plt.plot(subset['Hour'], subset['Total'], label=str(year))\n\nplt.title('Crimes by Year and Hour')\nplt.xlabel('Hour of the Day')\nplt.ylabel('Total Crimes')\nplt.legend(title='Year')\nplt.grid(True)\nplt.show()\n\n\n\n\nSeems like crimes are more likely to happend at night (from 10pm to 1am), it decreases afterwards and reaches the lowest point around 6am in the morning. During the day, it increases again.\n\n\nCrimes by Code, Crime type\n\nincidents['Dispatch_Date_Time'] = pd.to_datetime(incidents['Dispatch_Date_Time'])\nincidents['Year'] = incidents['Dispatch_Date_Time'].dt.year\nincidents['Hour'] = incidents['Dispatch_Date_Time'].dt.hour\n\n# Group by 'Text_General_Code' to get the total number of crimes for each code\nby_code = incidents.groupby('Text_General_Code').size().reset_index(name='Total')\nby_code = by_code.sort_values(by='Total', ascending=False)\n\n# Plotting top 10 Crimes by Code\nplt.figure(figsize=(9, 5.5))\nsns.barplot(x='Total', y='Text_General_Code', data=by_code.head(10))\nplt.title('Top 10 Crimes By Code')\nplt.xlabel('Total Crimes')\nplt.ylabel('Crime Text Code')\nplt.show()\n\n# Group by 'Year' and 'Text_General_Code' to get the total number of crimes for each code and year\nby_code_year = incidents.groupby(['Year', 'Text_General_Code']).size().reset_index(name='Total')\n\n# Plotting Crimes by Code and Year\nplt.figure(figsize=(9, 5.5))\nsns.barplot(x='Total', y='Text_General_Code', hue='Year', data=by_code_year, dodge=False)\nplt.title('Crimes By Code and Year')\nplt.xlabel('Total Crimes')\nplt.ylabel('Crime Text Code')\nplt.legend(title='Year', loc='lower right')\nplt.show()\n\n\n\n\n\n\n\nThefts are the most common type of crimes and it is increasing over years Vandalism is decreasing"
  },
  {
    "objectID": "analysis/crime-by-type-2023.html",
    "href": "analysis/crime-by-type-2023.html",
    "title": "Explorative Visualization and Analysis of 2023 Philadelphia Crime Data",
    "section": "",
    "text": "This notebook presents an explorative analysis of the 2023 crime data from Philadelphia. The analysis includes various types of incidents, with a focus on grouping them by incident type. We will also visualize the coordinates of each incident spot to observe spatial patterns and trends in the data. The goal is to gain deeper insights into the distribution and nature of crime incidents across different areas of Philadelphia.\nInspired by: Exploratory Data Analysis of Philadelphia Crime Data\n\nQuick Overview of Crime Data\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport numpy as np\n%matplotlib inline\nincidents = pd.read_csv('../data/incidents_part1_part2.csv')\nincidents.head()\n\n\n\n\n\n\n\n\nobjectid\ndc_dist\npsa\ndispatch_date_time\ndispatch_date\ndispatch_time\nhour\ndc_key\nlocation_block\nucr_general\ntext_general_code\npoint_x\npoint_y\nlat\nlng\n\n\n\n\n0\n151849\n22\nNaN\n2018-12-08 07:16:00+00\n2018-12-08\n02:16:00\n2.0\n2.018221e+11\nN 29th St & Ridge Ave\n300\nRobbery No Firearm\n-75.180807\n39.986800\n39.986800\n-75.180807\n\n\n1\n151850\n16\nNaN\n2018-07-12 01:26:00+00\n2018-07-11\n21:26:00\n21.0\n2.018160e+11\n400 BLOCK N 50th St\n400\nAggravated Assault No Firearm\n-75.220113\n39.964369\n39.964369\n-75.220113\n\n\n2\n151851\n14\nNaN\n2018-02-26 06:28:00+00\n2018-02-26\n01:28:00\n1.0\n2.018140e+11\nN 20th St & Wyncote Ave\n300\nRobbery Firearm\n-75.150925\n40.057370\n40.057370\n-75.150925\n\n\n3\n151853\n17\nNaN\n2018-02-20 01:02:00+00\n2018-02-19\n20:02:00\n20.0\n2.018170e+11\n2700 BLOCK Reed St\n400\nAggravated Assault Firearm\n-75.189692\n39.935738\n39.935738\n-75.189692\n\n\n4\n151854\n24\nNaN\n2018-09-20 23:31:00+00\n2018-09-20\n19:31:00\n19.0\n2.018241e+11\n3500 BLOCK FRANKFORD AVE\n500\nBurglary Residential\n-75.102406\n39.997427\n39.997427\n-75.102406\n\n\n\n\n\n\n\n\n\nCategorize Indicents into Various Types\n\ncrimes_names = ['Homicide', 'Aggravated', 'Rape', 'Drunkenness', 'Narcotic', 'Prostitution', 'Conduct', 'Fraud', 'Vandalism', 'Robbery', 'Burglary', 'Theft']\ncrimes_labels = ['Homicide', 'Aggravated Assault', 'Rape', 'Public Drunkenness', 'Drug Law Violations', 'Prostitution','Disorderly Conduct', 'Fraud', 'Vandalism','Robbery','Burglary','Theft']\n\n\n\nVisualize the Street Poles\n\ndata_poles = pd.read_csv('Street_Poles.csv')\nX_poles = data_poles['X'].values\nY_poles = data_poles['Y'].values\n\nplt.figure(figsize=(20,20))\nplt.scatter(X_poles,Y_poles,0.1,c='k')\nplt.xlim(-75.3,-74.9)\nplt.ylim(39.85,40.15)\nplt.axis('off')\n\n(-75.3, -74.9, 39.85, 40.15)\n\n\n\n\n\n\nimport numpy as np\nfrom sklearn.neighbors import KernelDensity\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Assuming incidents is a pandas DataFrame with the relevant data\ndata_crime = incidents\nmask_area = ((data_crime['point_x'] &gt; -75.3) & (data_crime['point_x'] &lt; -74.9) & \n             (data_crime['point_y'] &gt; 39.85) & (data_crime['point_y'] &lt; 40.15))\ndata_crime = data_crime[mask_area]\n\nmax_points = 20000\nxbins, ybins = 500, 500  # Set the number of bins as integers\nbandwidth = 0.001\n\n# Define a unique color for each type of crime\ncolors = plt.cm.get_cmap('viridis', len(crimes_names))\n\nfor k, crime in enumerate(crimes_names):\n    # Filter data by crime\n    crime_data = data_crime[data_crime['text_general_code'].str.contains(crime)]\n\n    # If there are more points than max_points, randomly sample\n    if len(crime_data) &gt; max_points:\n        crime_data = crime_data.sample(n=max_points)\n    \n    X_crime = crime_data['point_x'].values\n    Y_crime = crime_data['point_y'].values\n    XY_crime = np.array([X_crime, Y_crime]).T\n\n    # Create grid for KDE\n    x_min, x_max = X_crime.min(), X_crime.max()\n    y_min, y_max = Y_crime.min(), Y_crime.max()\n    x_grid = np.linspace(x_min, x_max, xbins)\n    y_grid = np.linspace(y_min, y_max, ybins)\n    xx, yy = np.meshgrid(x_grid, y_grid)\n    xy_sample = np.vstack([xx.ravel(), yy.ravel()]).T\n\n    # KDE\n    kde_skl = KernelDensity(bandwidth=bandwidth)\n    kde_skl.fit(XY_crime)\n\n    z = np.exp(kde_skl.score_samples(xy_sample))\n    zz = z.reshape(xx.shape)\n\n    plt.figure(figsize=(20, 20))\n    plt.scatter(X_poles, Y_poles, 0.1, c='k', alpha=0.6)\n    plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n    plt.xlim(-75.3, -74.9)\n    plt.ylim(39.85, 40.15)\n    plt.axis('off')\n    plt.title(f'{crime} 2023', size=33, color=colors(k))\n\nplt.show()\n\n\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:18: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  colors = plt.cm.get_cmap('viridis', len(crimes_names))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n/var/folders/tf/c5gvkllj6dn72kwhbbxxgcm40000gn/T/ipykernel_26683/3033318570.py:49: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmax' will be ignored\n  plt.scatter(X_crime, Y_crime, 7, c=colors(k), vmax=np.percentile(z, 95))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nincidents.head()\n\n\n\n\n\n\n\n\nobjectid\ndc_dist\npsa\ndispatch_date_time\ndispatch_date\ndispatch_time\nhour\ndc_key\nlocation_block\nucr_general\ntext_general_code\npoint_x\npoint_y\nlat\nlng\n\n\n\n\n0\n151849\n22\nNaN\n2018-12-08 07:16:00+00\n2018-12-08\n02:16:00\n2.0\n2.018221e+11\nN 29th St & Ridge Ave\n300\nRobbery No Firearm\n-75.180807\n39.986800\n39.986800\n-75.180807\n\n\n1\n151850\n16\nNaN\n2018-07-12 01:26:00+00\n2018-07-11\n21:26:00\n21.0\n2.018160e+11\n400 BLOCK N 50th St\n400\nAggravated Assault No Firearm\n-75.220113\n39.964369\n39.964369\n-75.220113\n\n\n2\n151851\n14\nNaN\n2018-02-26 06:28:00+00\n2018-02-26\n01:28:00\n1.0\n2.018140e+11\nN 20th St & Wyncote Ave\n300\nRobbery Firearm\n-75.150925\n40.057370\n40.057370\n-75.150925\n\n\n3\n151853\n17\nNaN\n2018-02-20 01:02:00+00\n2018-02-19\n20:02:00\n20.0\n2.018170e+11\n2700 BLOCK Reed St\n400\nAggravated Assault Firearm\n-75.189692\n39.935738\n39.935738\n-75.189692\n\n\n4\n151854\n24\nNaN\n2018-09-20 23:31:00+00\n2018-09-20\n19:31:00\n19.0\n2.018241e+11\n3500 BLOCK FRANKFORD AVE\n500\nBurglary Residential\n-75.102406\n39.997427\n39.997427\n-75.102406\n\n\n\n\n\n\n\n\nincidents=incidents.dropna()"
  }
]